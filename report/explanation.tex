\section{Explanation}

\subsection{Loop 1}

Loop 1 updates all of the values upper of the main diagonal (without the main diagonal) in a two-dimensional table. The given implementation assess more calculations to the outer loop's first iterations. In general, because of the small size of the problem monitor time is under half of a second, so the noise from the system makes a big difference in the timing.

\subsubsection{Scheduling option}
In terms of Loop 1 the best scheduler option running on 4 threads is (guided, 4) following by (guided, 8) and (guided, 1) with a slight running time difference of 0,001 and 0,004 seconds accordingly.

Dynamic and guided schedules perform better due to the fact that in this case, they take approximatelly the same load in total. For example, if thread 1 takes chunk 1 which contains a lot of calculations, that means whilst it is working on chunk 1 the other threads share the remaining load, not waiting for thread 1 to handle more chunks.

In contrast, static scheduling performs poorly because of the unbalanced loops. Assesing the "heavy" chunks to the first threads and the "light" to the last ones, makes for example, thread 0 and 1 to do more iterations than thread 2 and 3.

\subsubsection{Threads configuration}
We observe that Loop 1 performed better with 16 threads. To be more specific, as long as we use more threads the performarce of Loop 1 increases in a linear fashion. This makes sense because there are no data dependencies between different iterations of the loop. Also, as we increase the running threads on guided scheduling the chunksize becomes smaller. This means that the heavy workload (first outer loop iterations) will be divided fairly well to the active threads.

\subsection{Loop 2}

As regards Loop2 we can observe with a number of tests that it works heavily for the first iterations, due to the fact that for i's near to zero jmax[i] equals to N most of the times. In that case, nested loops perform more iterations. On the contrary, when i is big enough then jmax[i] becomes one (1) for the majority of the outer iterations, therefore the two nested loops are executed just once each.

\subsubsection{Scheduling option}
In terms of Loop 2, the fastest configuration was (dynamic, 16) followed by (dynamic, 8) and (static, 8). This is the reason why threads share the work load of both the heavy (small i's) and light (big i's) iterations.

On the one hand, static and dynamic are the fastest configurations due to the fact that they take the same chunksize to calculate. The only difference is that dynamic is a little bit faster because the chunks are not statically allocated to each threads. In case that, whenever a thread finishes its task it can allocate a new one from the available chunks.

On the other hand, guided scheduling takes the same time to be executed independent of the chunksize. This happens because the threads that will execute the first iterations will take the same work load in each case. Taking into consideration that the first iterations are computationally intense, that sums up to high and constant latency for this configuration.

\subsubsection{Threads configuration}
Figure 4 depicts that Loop 2 performed better with more than 6 threads, but also it had the same results for configurations such as 6, 8, 12 and 16 threads. This happened possibly because more than 6 threads are not required to improve performance in the specific problem. In this situation with schedule(dynamic, 16), 6 threads are enough to handle the "heavy" iterations. Adding more threads than that it will have no speedup impact because the work load is obsolete, so further distribution is not needed.